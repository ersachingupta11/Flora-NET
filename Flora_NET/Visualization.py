# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L6FA7tjzHbMW28CMKX-Ms8IZl1oTnuII

**Model**
"""

Flora_NET = FloraNET(num_classes=17)

"""**Feature Visualization**"""

def visualize_features(images_tensor, cmap='viridis', figsize=(20, 20)):
    num_images, num_channels, height, width = images_tensor.shape
    grid_size = int(np.ceil(np.sqrt(num_channels)))

    fig, axs = plt.subplots(grid_size, grid_size, figsize=figsize)

    for image_idx in range(num_images):
        for channel_idx in range(num_channels):
            ax = axs[channel_idx // grid_size, channel_idx % grid_size]
            channel_image = images_tensor[image_idx, channel_idx, :, :]
            ax.imshow(channel_image, cmap=cmap)
            # ax.set_title(f'Image {image_idx + 1}, Channel {channel_idx + 1}')
            ax.axis('off')

    # plt.tight_layout()
    plt.show()

with torch.no_grad():
  Flora_NET.eval()
  images, labels = next(iter(train_loader))
  features = images
  features = Flora_NET.extract_features(images)
  for layer_name, feature_map in features.items():
    print("Feature Map of Layer: ", layer_name)
    print("Intermediate features shape: ", feature_map.shape)
    visualize_features(feature_map)
  # visualize_features(features)

"""**Grad-CAM**"""

!pip install grad-cam
!pip install utils

import os
import PIL
import numpy as np
import torch
import torch.nn.functional as F
import torchvision.models as models
from torchvision.utils import make_grid, save_image

# from utils import visualize_cam, Normalize
# from gradcam import GradCAM, GradCAMpp

img_dir = '/kaggle/input/flower-dataset/flower_dataset/train/Albizia julibrissin/'
# img_name = 'collies.JPG'
# img_name = 'multiple_dogs.jpg'
# img_name = 'snake.JPEG'
img_name = 'Styphnolobium japonicum_flower_1 (113).jpg'
# img_path = os.path.join(img_dir, img_name)
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Aesculus chinensis/Aesculus chinensis_flower_1 (10).jpg'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Albizia julibrissin/Albizia julibrissin_1 (119).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Camptotheca acuminata/Camptotheca acuminata_flower_1 (122).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Flowering cherry/Flowering cherry_flower_1 (109).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Koelreuteria paniculata/Koelreuteria paniculata_flower_1 (111).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Lagerstroemia indica/Lagerstroemia indica_flower_1 (116).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Liriodendron chinense/Liriodendron chinense_flower_1 (25).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Loropetalum chinense var. rubrum/Loropetalum chinense var. rubrum_flower_1 (105).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Magnolia liliflora Desr/Magnolia liliflora Desr_flower_1 (18).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Malushalliana/Malushalliana_flower_1 (18).jpg'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Nandina domestica/Nandina domestica_flower_1 (19).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Nerium oleander L/Nerium oleander L._flower_1 (109).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Osmanthus fragrans/Osmanthus fragrans_flower_1 (109).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Photinia serratifolia/Photinia serratifolia_flower_1 (108).JPG'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Prunus persica/Prunus persica_flower_1 (124).jpg'
# img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Rhododendron pulchrum/Rhododendron pulchrum_flower_1 (13).JPG'
img_path = '/kaggle/input/flower-dataset/flower_dataset/train/Styphnolobium japonicum/Styphnolobium japonicum_flower_1 (113).JPG'

pil_img = PIL.Image.open(img_path)
pil_img

normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
torch_img = torch.from_numpy(np.asarray(pil_img)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()
torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False)
normed_torch_img = normalizer(torch_img)

from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
import copy
import cv2

Flora_NET = FloraNET(num_classes=17)
target_layers = [Flora_NET.inv1]
images, label = next(iter(train_loader))

input_tensor = images
# Create an input tensor image for your model..
# Note: input_tensor can be a batch tensor with several images!

# Construct the CAM object once, and then re-use it on many images:
cam = GradCAM(model=Flora_NET, target_layers=target_layers)

# You can also use it within a with statement, to make sure it is freed,
# In case you need to re-create it inside an outer loop:
# with GradCAM(model=model, target_layers=target_layers) as cam:
#   ...

# We have to specify the target we want to generate
# the Class Activation Maps for.
# If targets is None, the highest scoring category
# will be used for every image in the batch.
# Here we use ClassifierOutputTarget, but you can define your own custom targets
# That are, for example, combinations of categories, or specific outputs in a non standard model.

targets = [ClassifierOutputTarget(0)]

# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.
grayscale_cam = cam(input_tensor=input_tensor, targets=targets)

# In this example grayscale_cam has only one image in the batch:
grayscale_cam = grayscale_cam[0, :]
rgb_img = copy.deepcopy(images[0])

rgb_img_normalized = rgb_img.permute(1, 2, 0).numpy()  # Convert PyTorch tensor to numpy array
rgb_img_normalized /= 255.0  # Normalize to range [0, 1]
visualization = show_cam_on_image(rgb_img_normalized, grayscale_cam, use_rgb=True, colormap=cv2.COLORMAP_HOT)
# visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)
print(visualization.shape)

# You can also get the model outputs without having to re-inference
model_outputs = cam.outputs

"""**t-SNE Plot**"""

tsne = TSNE(n_components=3)
tsne_result = tsne.fit_transform(pca_result)
tsne_result_scaled = StandardScaler().fit_transform(tsne_result)

from mpl_toolkits.mplot3d import Axes3D
from matplotlib import animation

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111,projection='3d')

plt.grid()

nb_classes = len(np.unique(label_ids))

for label_id in np.unique(label_ids):
    ax.scatter(tsne_result_scaled[np.where(label_ids == label_id), 0],
                tsne_result_scaled[np.where(label_ids == label_id), 1],
                tsne_result_scaled[np.where(label_ids == label_id), 2],
                alpha=0.8,
                color= plt.cm.Set1(label_id / float(nb_classes)),
                marker='o',
                label=id_to_label_dict[label_id])
ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
ax.view_init(25, 45)
ax.set_xlim(-2.5, 2.5)
ax.set_ylim(-2.5, 2.5)
ax.set_zlim(-2.5, 2.5)

"""**Confusion Matrix**"""

import torch
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming Flora_NET model is already defined and loaded
# And test_loader is the DataLoader for your test dataset

# Initialize lists to store predictions and true labels
all_preds = []
all_labels = []

# Set the model to evaluation mode
Flora_NET.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = Flora_NET(images)

        # Get predicted class for each output
        _, preds = torch.max(outputs, 1)

        # Append predictions and true labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_preds)

# Visualize the confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Oranges", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix for Flora_NET")
plt.show()
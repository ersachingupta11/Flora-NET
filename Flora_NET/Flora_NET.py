# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L6FA7tjzHbMW28CMKX-Ms8IZl1oTnuII

**CBS Block**
"""

import torch
import torch.nn as nn

class CBS(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1):
        super(CBS, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0)
        self.bn = nn.BatchNorm2d(out_channels)
        self.silu = nn.SiLU()

    def forward(self, x):
        return self.silu(self.bn(self.conv(x)))

"""**DCAFE Module**"""

import torch
import torch.nn as nn
import math
import torch.nn.functional as F

class CoordAttMeanMax(nn.Module):
    def __init__(self, inp, oup, groups=32):
        super(CoordAttMeanMax, self).__init__()
        self.pool_h_mean = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w_mean = nn.AdaptiveAvgPool2d((1, None))
        self.pool_h_max = nn.AdaptiveMaxPool2d((None, 1))
        self.pool_w_max = nn.AdaptiveMaxPool2d((1, None))

        mip = max(8, inp // groups)

        self.conv1_mean = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1_mean = nn.BatchNorm2d(mip)
        self.conv2_mean = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)

        self.conv1_max = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1_max = nn.BatchNorm2d(mip)
        self.conv2_max = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()

        # Mean pooling branch
        x_h_mean = self.pool_h_mean(x)
        x_w_mean = self.pool_w_mean(x).permute(0, 1, 3, 2)
        y_mean = torch.cat([x_h_mean, x_w_mean], dim=2)
        y_mean = self.conv1_mean(y_mean)
        y_mean = self.bn1_mean(y_mean)
        y_mean = self.relu(y_mean)
        x_h_mean, x_w_mean = torch.split(y_mean, [h, w], dim=2)
        x_w_mean = x_w_mean.permute(0, 1, 3, 2)

        # Max pooling branch
        x_h_max = self.pool_h_max(x)
        x_w_max = self.pool_w_max(x).permute(0, 1, 3, 2)
        y_max = torch.cat([x_h_max, x_w_max], dim=2)
        y_max = self.conv1_max(y_max)
        y_max = self.bn1_max(y_max)
        y_max = self.relu(y_max)
        x_h_max, x_w_max = torch.split(y_max, [h, w], dim=2)
        x_w_max = x_w_max.permute(0, 1, 3, 2)

        # Apply attention
        x_h_mean = self.conv2_mean(x_h_mean).sigmoid()
        x_w_mean = self.conv2_mean(x_w_mean).sigmoid()
        x_h_max = self.conv2_max(x_h_max).sigmoid()
        x_w_max = self.conv2_max(x_w_max).sigmoid()

        # Expand to original shape
        x_h_mean = x_h_mean.expand(-1, -1, h, w)
        x_w_mean = x_w_mean.expand(-1, -1, h, w)
        x_h_max = x_h_max.expand(-1, -1, h, w)
        x_w_max = x_w_max.expand(-1, -1, h, w)

        # Combine outputs
        attention_mean = identity * x_w_mean * x_h_mean
        attention_max = identity * x_w_max * x_h_max

        # Sum the attention outputs
        return attention_mean + attention_max

class DCAFE(nn.Module):
    def __init__(self, in_channels):
        super(DCAFE, self).__init__()
        self.coord_att = CoordAttMeanMax(in_channels, out_channels=1260)

    def forward(self, x):
        return self.coord_att(x)

# Example model incorporating the DCAFE module
class FloraNETDCAFE(nn.Module):
    def __init__(self, num_classes=17):
        super(FloraNETDCAFE, self).__init__()

        # First four CBS layers (as previously defined)
        self.layer1 = conv_3x3_bn(3, 128, 2)
        self.layer2 = conv_1x1_bn(128, 128)
        self.layer3 = conv_1x1_bn(128, 128)
        self.layer4 = conv_1x1_bn(128, 128)

        # DCAFE module
        self.dca = DCAFE(in_channels=128)

        # Final layers
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.dca(x)  # Apply DCAFE module
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

"""**Inv-FR Module**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class InvolutionLayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1):
        super(InvolutionLayer, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.padding = padding
        self.stride = stride

        # Define a convolution layer that will be used to generate the kernel
        self.kernel_generator = nn.Conv2d(in_channels, in_channels, kernel_size, padding=padding, groups=in_channels, bias=False)

    def forward(self, x):
        # Get the batch size and spatial dimensions
        batch_size, _, height, width = x.size()

        # Generate the kernel from the input
        kernel = self.kernel_generator(x)

        # Reshape the kernel to be used for convolution
        kernel = kernel.view(batch_size, self.in_channels, self.kernel_size, self.kernel_size, height, width)

        # Perform convolution with the generated kernel
        output = F.conv2d(x.view(batch_size, self.in_channels, -1).permute(0, 2, 1).view(batch_size * height * width, self.in_channels, 1, 1),
                          kernel.view(batch_size * height * width, self.in_channels, self.kernel_size, self.kernel_size),
                          stride=self.stride, padding=self.padding)

        # Reshape the output to (batch_size, out_channels, height, width)
        output = output.view(batch_size, height, width, self.out_channels).permute(0, 3, 1, 2)

        return output

class Involution(nn.Module):
    def __init__(self):
        super(Involution, self).__init__()
        self.involution1 = InvolutionLayer(in_channels=128, out_channels=512, kernel_size=1)
        self.involution2 = InvolutionLayer(in_channels=512, out_channels=1024, kernel_size=1)

    def forward(self, x):
        x = self.involution1(x)  # Apply first Involution layer
        x = self.involution2(x)  # Apply second Involution layer
        return x

# Example usage
class FloraNETINVFR(nn.Module):
    def __init__(self, num_classes=17):
        super(FloraNETINVFR, self).__init__()

        # Initial convolutional layers (CBS) for demonstration
        self.layer1 = conv_3x3_bn(3, 128, 2)
        self.layer2 = conv_1x1_bn(128, 128)

        # Involution module
        self.involution = Involution()

        # Final layers
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(1024, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.involution(x)  # Apply Involution module
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

"""**Cascaded Convolution**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ConvBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)
        self.silu = nn.SiLU()  # SiLU Activation

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.bn(x)
        x = self.silu(x)
        return x

class CascadedBlock(nn.Module):
    def __init__(self, in_channels):
        super(CascadedBlock, self).__init__()

        # First cascade
        self.cascade1 = ConvBlock(in_channels, 256)  # Adjust output channels as necessary
        # Second cascade
        self.cascade2 = ConvBlock(256, 512)  # Output channels must match the input channels for the next block
        # Third cascade
        self.cascade3 = ConvBlock(512, 512)  # Further processing

        # Final 1x1 convolution to generate 1024 output channels
        self.final_conv = nn.Conv2d(512, 1024, kernel_size=1)

    def forward(self, x):
        x = self.cascade1(x)  # First cascade
        x = self.cascade2(x)  # Second cascade
        x = self.cascade3(x)  # Third cascade
        x = self.final_conv(x)  # Final 1x1 convolution
        return x


class FloraNETCascaded(nn.Module):
    def __init__(self, num_classes=17):
        super(FloraNETCascaded, self).__init__()

        # Initial convolutional layers
        self.layer1 = nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1)  # Initial layer
        self.layer2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)

        # Cascaded block
        self.cascaded_block = CascadedBlock(128)

        # Final pooling and classification
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(1024, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.cascaded_block(x)  # Apply the Cascaded Block
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

"""**Classification Module**"""

import torch
import torch.nn as nn

class ClassificationBlock(nn.Module):
    def __init__(self, num_classes=17):
        super(ClassificationBlock, self).__init__()
        self.maxpool = nn.MaxPool2d(kernel_size=2)  # Max pooling layer
        self.fc1 = nn.Linear(1024, 512)  # Fully connected layer from the last conv output size
        self.fc2 = nn.Linear(512, 1024)   # Second fully connected layer
        self.fc3 = nn.Linear(1024, num_classes)  # Output layer for class predictions

    def forward(self, x):
        x = self.maxpool(x)  # Apply max pooling
        x = torch.flatten(x, 1)  # Flatten the tensor for fully connected layers
        x = self.fc1(x)  # First fully connected layer
        x = nn.ReLU()(x)  # Activation after first fc layer
        x = self.fc2(x)  # Second fully connected layer
        x = nn.ReLU()(x)  # Activation after second fc layer
        x = self.fc3(x)  # Final output layer
        return x

# Example usage
class FloraNETClassification(nn.Module):
    def __init__(self, num_classes=17):
        super(FloraNETClassification, self).__init__()

        # Example initial convolutional layers, you can modify as needed
        self.layer1 = nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1)  # Initial layer
        self.layer2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        # Classification Block
        self.classification_block = ClassificationBlock(num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.classification_block(x)  # Apply the Classification Block
        return x

"""**Importing the Command Line Arguments**"""
import sys
# Access command-line arguments directly
num_epoch = int(sys.argv[1])
num_lr = float(sys.argv[2])
num_batch = int(sys.argv[3])

"""**Flora-NET Model**"""

import torch
import torch.nn as nn

class FloraNET(nn.Module):
    def __init__(self, num_classes=17):
        super(CompleteModel, self).__init__()

        # CBS Block
        self.cbs1 = CBS(input_channels=3, num_filters=128)  # First CBS Block
        self.cbs2 = CBS(input_channels=128, num_filters=128)  # Second CBS Block
        self.cbs3 = CBS(input_channels=128, num_filters=128)  # Third CBS Block
        self.cbs4 = CBS(input_channels=128, num_filters=128)  # Fourth CBS Block

        # DCAFE Module
        self.dcafe = FloraNETDCAFE(in_channels=128, out_channels=1260)  # DCAFE Module

        # Involution Module
        self.involution = FloraNETINVFR()  # Custom Involution class with two layers

        # Cascaded Block
        self.cascaded_block = FloraNETCascaded()  # Custom CascadedBlock class

        # Classification Block
        self.classification_block = FloraNETClassification(num_classes=num_classes)  # Final Classification Block

    def forward(self, x):
        x = self.cbs1(x)  # Apply CBS Block 1
        x = self.cbs2(x)  # Apply CBS Block 2
        x = self.cbs3(x)  # Apply CBS Block 3
        x = self.cbs4(x)  # Apply CBS Block 4

        x = self.dcafe(x)  # Apply DCAFE Module
        x = self.involution(x)  # Apply Involution Layer

        x = self.cascaded_block(x)  # Apply Cascaded Block
        x = self.classification_block(x)  # Apply Classification Block

        return x

# Instantiate the complete model
Flora_NET = FloraNET(num_classes=17)

# Check the model architecture
print(Flora_NET)
